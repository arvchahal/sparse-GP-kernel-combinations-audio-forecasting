{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple 2D Sparse GP\n",
    "We will test to ensure that our sparse GP implementation works correctly in this notebook.\n",
    "\n",
    "## Generate Syntethic Data\n",
    "We generate a simple dataset for testing with inputs in the range [0, 200]. We'll use a sinusoidal function with some noise added."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pandas'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 8\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mimageio\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdata\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkernels\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msparse_gp\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n",
      "File \u001b[0;32m~/Downloads/sparse-GP-kernel-combinations-audio-forecasting/data.py:2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mjax\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# simple functions for loading the different data groups\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \n\u001b[1;32m      6\u001b[0m \n\u001b[1;32m      7\u001b[0m \n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m#returns only the soccer team np array\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'pandas'"
     ]
    }
   ],
   "source": [
    "import jax\n",
    "import jax.numpy as np\n",
    "import numpy as onp\n",
    "import matplotlib.pyplot as plt\n",
    "from jax.scipy.linalg import cho_factor, cho_solve\n",
    "import imageio\n",
    "import os\n",
    "from data import *\n",
    "\n",
    "from kernels import *\n",
    "from sparse_gp import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load datasets\n",
    "all_data = load_all()\n",
    "tech = load_tech()\n",
    "politics = load_politics()\n",
    "soccer = load_soccer()\n",
    "\n",
    "# Example: Split Soccer data into training and testing sets\n",
    "X_train_soccer, X_test_soccer, y_train_soccer, y_test_soccer = split_train_test_matrix(soccer, train_ratio=0.75, target_column=0)\n",
    "\n",
    "# Generate integer day labels\n",
    "time_train = np.arange(X_train_soccer.shape[0])  # Days for training set\n",
    "time_test = np.arange(X_train_soccer.shape[0], X_train_soccer.shape[0] + X_test_soccer.shape[0])  # Days for testing set\n",
    "\n",
    "# Soccer group feature names (e.g., team names)\n",
    "soccer_feature_names = [row[0] for row in soccer if row[0] != soccer[0, 0]]\n",
    "\n",
    "# Plot all soccer teams in the training set\n",
    "plot_time_series(\n",
    "    X=X_train_soccer, \n",
    "    Y=y_train_soccer, \n",
    "    time=time_train, \n",
    "    feature_names=soccer_feature_names, \n",
    "    target_name=\"Manchester City\",  # Example target\n",
    "    title=\"Soccer Data - Training Set (All Teams)\"\n",
    ")\n",
    "\n",
    "# Plot all soccer teams in the testing set\n",
    "plot_time_series(\n",
    "    X=X_test_soccer, \n",
    "    Y=y_test_soccer, \n",
    "    time=time_test, \n",
    "    feature_names=soccer_feature_names, \n",
    "    target_name=\"Manchester City\",  # Example target\n",
    "    title=\"Soccer Data - Testing Set (All Teams)\"\n",
    ")\n",
    "\n",
    "# Example: Repeat for Tech data\n",
    "X_train_tech, X_test_tech, y_train_tech, y_test_tech = split_train_test_matrix(tech, train_ratio=0.75, target_column=0)\n",
    "\n",
    "# Generate integer day labels for Tech\n",
    "time_train_tech = np.arange(X_train_tech.shape[0])\n",
    "time_test_tech = np.arange(X_train_tech.shape[0], X_train_tech.shape[0] + X_test_tech.shape[0])\n",
    "\n",
    "# Tech group feature names (e.g., companies)\n",
    "tech_feature_names = [row[0] for row in tech if row[0] != tech[0, 0]]\n",
    "\n",
    "# Plot all tech companies in the training set\n",
    "plot_time_series(\n",
    "    X=X_train_tech, \n",
    "    Y=y_train_tech, \n",
    "    time=time_train_tech, \n",
    "    feature_names=tech_feature_names, \n",
    "    target_name=\"Google\",  # Example target\n",
    "    title=\"Tech Data - Training Set (All Companies)\"\n",
    ")\n",
    "\n",
    "# Plot all tech companies in the testing set\n",
    "plot_time_series(\n",
    "    X=X_test_tech, \n",
    "    Y=y_test_tech, \n",
    "    time=time_test_tech, \n",
    "    feature_names=tech_feature_names, \n",
    "    target_name=\"Google\",  # Example target\n",
    "    title=\"Tech Data - Testing Set (All Companies)\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inputs\n",
    "X = onp.linspace(0, 200, 300).reshape(-1, 1)\n",
    "\n",
    "# Outputs: Sine function with noise\n",
    "Y = onp.sin(X * 0.05) + 0.05 * onp.random.normal(size=X.shape)\n",
    "\n",
    "# Training and testing split\n",
    "X_train, Y_train = X[:175], Y[:175]\n",
    "X_test, Y_test = X[175:], Y[175:]\n",
    "\n",
    "# Plot data\n",
    "plt.scatter(X_train, Y_train, label=\"Train Points\")\n",
    "plt.scatter(X_test, Y_test, label=\"Test Points\", color=\"red\")\n",
    "plt.xlabel(\"X\")\n",
    "plt.ylabel(\"Y\")\n",
    "plt.legend()\n",
    "plt.title(\"Synthetic 1D Data with Train/Test Split\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting Code\n",
    "### Predictive Mean and Uncertainty Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_sparse_gp_with_uncertainty(X_train, Y_train, X_test, Y_test, Z, optimized_hyperparams, model_fn):\n",
    "    # Ensure input arrays are two-dimensional\n",
    "    X_test = X_test.reshape(-1, 1)\n",
    "    X_train = X_train.reshape(-1, 1)\n",
    "    Z = Z.reshape(-1, 1)\n",
    "    numInducing = Z.shape[0]\n",
    "\n",
    "    # Make predictions on the test set using the optimized hyperparameters\n",
    "    posterior_mean, posterior_var = model_fn(X_test, X_train, Y_train, Z, optimized_hyperparams)\n",
    "\n",
    "    # Convert JAX arrays to NumPy arrays for plotting and ensure all are 1D\n",
    "    posterior_mean = onp.array(posterior_mean).flatten()\n",
    "    posterior_var = onp.array(posterior_var).flatten()\n",
    "    X_test = onp.array(X_test).flatten()\n",
    "\n",
    "    # Ensure all variances are positive\n",
    "    posterior_var = onp.maximum(posterior_var, 1e-10)\n",
    "\n",
    "    # Plot the points, predictive mean, and confidence interval\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.scatter(X_train, Y_train, color='blue', label=\"Training Points\")\n",
    "    plt.scatter(X_test, Y_test, color='red', label=\"Test Points\")\n",
    "\n",
    "    # Plot predictive mean line and 95% confidence interval\n",
    "    plt.plot(X_test, posterior_mean, 'green', label=\"Predicted Mean\")\n",
    "    plt.fill_between(X_test,\n",
    "                     (posterior_mean - 1.96 * onp.sqrt(posterior_var)),\n",
    "                     (posterior_mean + 1.96 * onp.sqrt(posterior_var)),\n",
    "                     color='green', alpha=0.2, label=\"95% Confidence Interval\")\n",
    "\n",
    "    plt.xlabel(\"X\")\n",
    "    plt.ylabel(\"Y\")\n",
    "    plt.legend(loc=\"upper center\", bbox_to_anchor=(0.5, -0.15), ncol=3, frameon=False)\n",
    "    plt.title(f\"Sparse GP Prediction with Uncertainty (Including {numInducing} Inducing Points)\")\n",
    "    plt.show()\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Negative Log Predictive Density Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_nlpd(X, Y, Z, hyperparams, model_fn, noise_variance):\n",
    "    posterior_mean, posterior_var = model_fn(X, X_train, Y_train, Z, hyperparams)\n",
    "    nlpd = neg_log_predictive_density(Y, posterior_mean, posterior_var, noise_variance)\n",
    "    return nlpd\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ELBO Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_elbo(history):\n",
    "    # Extract ELBO values and steps from history\n",
    "    elbo_values = [entry[\"elbo\"] for entry in history]\n",
    "    steps = [entry[\"step\"] for entry in history]\n",
    "    \n",
    "    # Plot the ELBO values over steps\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(steps, elbo_values, label=\"ELBO\", color='blue')\n",
    "    plt.xlabel(\"Step\")\n",
    "    plt.ylabel(\"ELBO\")\n",
    "    plt.title(\"ELBO Over Training Steps\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kernel Hyperparameter Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_kernel_hyperparameters(history):\n",
    "    # Extract training steps\n",
    "    steps = [entry[\"step\"] for entry in history]\n",
    "    \n",
    "    # Function to normalize values between 0 and 1\n",
    "    def normalize(values):\n",
    "        min_val, max_val = min(values), max(values)\n",
    "        return [(v - min_val) / (max_val - min_val) for v in values] if max_val > min_val else values\n",
    "\n",
    "    # Extract and normalize raw hyperparameters\n",
    "    weights_1 = normalize([entry[\"hyperparams\"][0] for entry in history])\n",
    "    weights_2 = normalize([entry[\"hyperparams\"][1] for entry in history])\n",
    "    weights_3 = normalize([entry[\"hyperparams\"][2] for entry in history])\n",
    "    noise_variance_sqexp = normalize([entry[\"hyperparams\"][3] for entry in history])\n",
    "    signal_variance_sqexp = normalize([entry[\"hyperparams\"][4] for entry in history])\n",
    "    length_scale_sqexp = normalize([entry[\"hyperparams\"][5] for entry in history])\n",
    "    noise_variance_linear = normalize([entry[\"hyperparams\"][6] for entry in history])\n",
    "    signal_variance_linear = normalize([entry[\"hyperparams\"][7] for entry in history])\n",
    "    noise_variance_matern = normalize([entry[\"hyperparams\"][8] for entry in history])\n",
    "    signal_variance_matern = normalize([entry[\"hyperparams\"][9] for entry in history])\n",
    "    length_scale_matern = normalize([entry[\"hyperparams\"][10] for entry in history])\n",
    "    nu_matern = normalize([entry[\"hyperparams\"][11] for entry in history])\n",
    "\n",
    "    \n",
    "    # Plot normalized kernel hyperparameters\n",
    "    plt.figure(figsize=(12, 8))\n",
    "\n",
    "    # Plot for Squared-Exponential Kernel (Blue)\n",
    "    plt.plot(steps, weights_1, label=\"Weight (Squared-Exponential Kernel)\", color='blue', linestyle='--')\n",
    "    plt.plot(steps, noise_variance_sqexp, label=\"Noise Variance (Sq-Exp)\", color='blue', linestyle='-.')\n",
    "    plt.plot(steps, signal_variance_sqexp, label=\"Signal Variance (Sq-Exp)\", color='blue', linestyle='-')\n",
    "    plt.plot(steps, length_scale_sqexp, label=\"Length Scale (Sq-Exp)\", color='blue', linestyle=':')\n",
    "\n",
    "    # Plot for Linear Kernel (Red)\n",
    "    plt.plot(steps, weights_2, label=\"Weight (Linear Kernel)\", color='red', linestyle='--')\n",
    "    plt.plot(steps, noise_variance_linear, label=\"Noise Variance (Linear)\", color='red', linestyle='-.')\n",
    "    plt.plot(steps, signal_variance_linear, label=\"Signal Variance (Linear)\", color='red', linestyle='-')\n",
    "\n",
    "    # Plot for Matern Kernel (Green)\n",
    "    plt.plot(steps, weights_3, label=\"Weight (Matern Kernel)\", color='green', linestyle='--')\n",
    "    plt.plot(steps, noise_variance_matern, label=\"Noise Variance (Matern)\", color='green', linestyle='-.')\n",
    "    plt.plot(steps, signal_variance_matern, label=\"Signal Variance (Matern)\", color='green', linestyle='-')\n",
    "    plt.plot(steps, length_scale_matern, label=\"Length Scale (Matern)\", color='green', linestyle=':')\n",
    "    plt.plot(steps, nu_matern, label=\"Nu (Matern)\", color='green', linestyle='--')\n",
    "\n",
    "    # Plot configuration\n",
    "    plt.xlabel(\"Step\")\n",
    "    plt.ylabel(\"Normalized Parameter Value (0 to 1)\")\n",
    "    plt.title(\"Normalized Kernel Hyperparameter Values Over Training Steps\")\n",
    "    plt.legend(loc=\"upper left\", bbox_to_anchor=(1.05, 1), borderaxespad=0.)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Experiment 1\n",
    "Test on 20 inducing points.\n",
    "\n",
    "## Parameter Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize PRNG\n",
    "init_prng(0)\n",
    "\n",
    "# Initialize inducing points as a subset of X_train\n",
    "Z = initialize_inducing_points(X_train, num_inducing=20)\n",
    "\n",
    "# Define a function to initialize hyperparameters for the combined kernel\n",
    "def random_init_combined_hyperparams():\n",
    "    # Three initial weights in log space for unconstrained optimization\n",
    "    initial_weights = [np.log(0.33), np.log(0.33), np.log(0.33)]\n",
    "    \n",
    "    # Initial hyperparameters for each kernel\n",
    "    hyperparams_sqexp = [0.1, 1.0, 10.0]        # [noise_variance, signal_variance, length_scale]\n",
    "    hyperparams_linear = [0.1, 1.0]             # [noise_variance, signal_variance]\n",
    "    hyperparams_matern = [0.1, 1.0, 10.0, 2.5]  # [noise_variance, signal_variance, length_scale, nu]\n",
    "\n",
    "    # Concatenate all hyperparameters\n",
    "    initial_params = initial_weights + hyperparams_sqexp + hyperparams_linear + hyperparams_matern\n",
    "    return np.array(initial_params)\n",
    "#\n",
    "\n",
    "# Initialize hyperparameters in the unconstrained space\n",
    "unconstrained_hyperparams_init = random_init_combined_hyperparams()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the Optimization Loop for ELBO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the gradient ascent step size and number of steps\n",
    "step_size = 5e-3\n",
    "num_steps = 500\n",
    "\n",
    "# Optimize ELBO using the empirical Bayes function\n",
    "optimized_hyperparams, final_elbo, history = empirical_bayes(X_train, Y_train, Z, unconstrained_hyperparams_init, step_size, num_steps)\n",
    "\n",
    "print(\"Optimized Hyperparameters:\", optimized_hyperparams)\n",
    "print(\"Final ELBO Value:\", final_elbo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Optimized Hyperparameters:\", optimized_hyperparams)\n",
    "print(\"sum of First 3 Weights:\", np.sum(optimized_hyperparams[:3]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict and Plot the Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_sparse_gp_with_uncertainty(X_train, Y_train, X_test, Y_test, Z, optimized_hyperparams, sparse_gp_posterior_predictive)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract weights\n",
    "weights = optimized_hyperparams[:3]\n",
    "weight_percentages = (weights / np.sum(weights)) * 100\n",
    "\n",
    "# Print weights as percentages for each kernel\n",
    "print(\"Kernel Weights as Percentages:\")\n",
    "print(f\"Kernel 1 (Squared-Exponential): {weight_percentages[0]:.2f}%\")\n",
    "print(f\"Kernel 2 (Linear): {weight_percentages[1]:.2f}%\")\n",
    "print(f\"Kernel 3 (Matérn): {weight_percentages[2]:.2f}%\")\n",
    "\n",
    "# Print other hyperparameters for each kernel\n",
    "print(\"\\nOther Hyperparameters:\")\n",
    "print(f\"Kernel 1 - Noise Variance: {optimized_hyperparams[3]:.4f}\")\n",
    "print(f\"Kernel 1 - Signal Variance: {optimized_hyperparams[4]:.4f}\")\n",
    "print(f\"Kernel 1 - Length Scale: {optimized_hyperparams[5]:.4f}\")\n",
    "print(f\"Kernel 2 - Noise Variance: {optimized_hyperparams[6]:.4f}\")\n",
    "print(f\"Kernel 2 - Signal Variance: {optimized_hyperparams[7]:.4f}\")\n",
    "print(f\"Kernel 3 - Noise Variance: {optimized_hyperparams[8]:.4f}\")\n",
    "print(f\"Kernel 3 - Signal Variance: {optimized_hyperparams[9]:.4f}\")\n",
    "print(f\"Kernel 3 - Length Scale: {optimized_hyperparams[10]:.4f}\")\n",
    "print(f\"Kernel 3 - Nu: {optimized_hyperparams[11]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NLPD on Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_nlpd = calculate_nlpd(X_train, Y_train, Z, optimized_hyperparams, sparse_gp_posterior_predictive, noise_variance=optimized_hyperparams[2])\n",
    "test_nlpd = calculate_nlpd(X_test, Y_test, Z, optimized_hyperparams, sparse_gp_posterior_predictive, noise_variance=optimized_hyperparams[2])\n",
    "\n",
    "print(f\"Train NLPD: {train_nlpd:.4f}\")\n",
    "print(f\"Test NLPD: {test_nlpd:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ELBO Maximization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_elbo(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kernel Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_kernel_hyperparameters(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Experiment 2\n",
    "Test on 100 inducing points.\n",
    "\n",
    "## Parameter Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize PRNG\n",
    "init_prng(0)\n",
    "\n",
    "# Initialize inducing points as a subset of X_train\n",
    "Z = initialize_inducing_points(X_train, num_inducing=100)\n",
    "\n",
    "# Initialize hyperparameters in the unconstrained space\n",
    "unconstrained_hyperparams_init = random_init_combined_hyperparams()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the Optimization Loop for ELBO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the gradient ascent step size and number of steps\n",
    "step_size = 5e-3\n",
    "num_steps = 500\n",
    "\n",
    "# Optimize ELBO using the empirical Bayes function\n",
    "optimized_hyperparams, final_elbo, history = empirical_bayes(X_train, Y_train, Z, unconstrained_hyperparams_init, step_size, num_steps)\n",
    "\n",
    "print(\"Optimized Hyperparameters:\", optimized_hyperparams)\n",
    "print(\"Final ELBO Value:\", final_elbo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Optimized Hyperparameters:\", optimized_hyperparams)\n",
    "print(\"sum of First 2 Weights:\", np.sum(optimized_hyperparams[:2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict and Plot the Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_sparse_gp_with_uncertainty(X_train, Y_train, X_test, Y_test, Z, optimized_hyperparams, sparse_gp_posterior_predictive)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract weights\n",
    "weights = optimized_hyperparams[:3]\n",
    "weight_percentages = (weights / np.sum(weights)) * 100\n",
    "\n",
    "# Print weights as percentages for each kernel\n",
    "print(\"Kernel Weights as Percentages:\")\n",
    "print(f\"Kernel 1 (Squared-Exponential): {weight_percentages[0]:.2f}%\")\n",
    "print(f\"Kernel 2 (Linear): {weight_percentages[1]:.2f}%\")\n",
    "print(f\"Kernel 3 (Matérn): {weight_percentages[2]:.2f}%\")\n",
    "\n",
    "# Print other hyperparameters for each kernel\n",
    "print(\"\\nOther Hyperparameters:\")\n",
    "print(f\"Kernel 1 - Noise Variance: {optimized_hyperparams[3]:.4f}\")\n",
    "print(f\"Kernel 1 - Signal Variance: {optimized_hyperparams[4]:.4f}\")\n",
    "print(f\"Kernel 1 - Length Scale: {optimized_hyperparams[5]:.4f}\")\n",
    "print(f\"Kernel 2 - Noise Variance: {optimized_hyperparams[6]:.4f}\")\n",
    "print(f\"Kernel 2 - Signal Variance: {optimized_hyperparams[7]:.4f}\")\n",
    "print(f\"Kernel 3 - Noise Variance: {optimized_hyperparams[8]:.4f}\")\n",
    "print(f\"Kernel 3 - Signal Variance: {optimized_hyperparams[9]:.4f}\")\n",
    "print(f\"Kernel 3 - Length Scale: {optimized_hyperparams[10]:.4f}\")\n",
    "print(f\"Kernel 3 - Nu: {optimized_hyperparams[11]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NLPD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_nlpd = calculate_nlpd(X_train, Y_train, Z, optimized_hyperparams, sparse_gp_posterior_predictive, noise_variance=optimized_hyperparams[2])\n",
    "test_nlpd = calculate_nlpd(X_test, Y_test, Z, optimized_hyperparams, sparse_gp_posterior_predictive, noise_variance=optimized_hyperparams[2])\n",
    "\n",
    "print(f\"Train NLPD: {train_nlpd:.4f}\")\n",
    "print(f\"Test NLPD: {test_nlpd:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ELBO Maximization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_elbo(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kernel Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_kernel_hyperparameters(history)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "advML",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
